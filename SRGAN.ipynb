{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프로젝트 1 : 직접 고른 이미지로 SRGAN 실험하기   \n",
    "\n",
    "이제 여러분이 직접 다른 이미지를 사용해 볼 차례입니다.\n",
    "\n",
    "앞서 사용했던 DIV2K 데이터셋은 bicubic interpolation을 이용해 가로 및 세로 픽셀 수를 1/4로 줄인 저해상도 이미지와 원래 고해상도 이미지 사이에서 학습되었습니다. 이 데이터셋으로 학습된 SRGAN를 이용해 좋은 결과를 얻기 위해서는 위와 같은 과정이 동일하게 진행되는 것이 좋습니다.\n",
    "\n",
    "아래와 같이 두 가지 다른 단계를 거쳐서 직접 고른 이미지에 대해 SRGAN을 적용해 봅시다. (모델 학습을 진행하는 건 아닙니다❗) 테스트할 이미지를 고를 때, DIV2K 의 학습 데이터셋과 비슷한 종류의 이미지가 좋습니다. 일반적으로 자연, 동물, 건물 등의 이미지가 학습에 사용되었으며, 이와 달리 애니메이션 등의 이미지는 고해상도로 잘 변환되지 않을 수 있습니다.\n",
    "\n",
    "특정 데이터셋에서만 학습되었기 때문에, 일반적으로 좋지 않은 결과가 나올 가능성이 높습니다. 결과와 상관없이 아래 과정만 잘 수행해봅시다.\n",
    "\n",
    "### 프로젝트 1-1.\n",
    "\n",
    "1. (적당히) 높은 해상도를 가진 이미지를 검색해서 한 장 고른 후 저장하고 불러옵니다.   \n",
    "\n",
    "2. 불러온 이미지에 bicubic interpolation을 적용해 가로 및 세로 픽셀 수를 1/4로 줄입니다. cv2.resize()를 사용해 봅시다.   \n",
    "\n",
    "3. 줄인 저해상도 이미지를 입력으로 SRGAN을 이용해 고해상도 이미지를 생성합니다. 이전에 사용한 apply_srgan 함수를 사용하면 쉽습니다.\n",
    "4. 2.의 이미지에 bicubic interpolation을 적용해 가로 및 세로 픽셀 수를 다시 4배로 늘립니다. 마찬가지로 cv2.resize()를 사용해 봅시다.\n",
    "5. 3개 이미지(4.의 Bicubic의 결과, 3.의 SRGAN의 결과, 1.의 원래 고해상도 이미지)를 나란히 시각화합니다. 각 이미지의 제목에 어떤 방법에 대한 결과인지 표시해 주세요. 이전 시각화에 사용했던 코드를 참고하면 어렵지 않습니다.\n",
    "6. 선택한 이미지를 DIV2K 데이터셋에서 학습된 모델로 Super Resolution했을 때 어떠한 결과가 나왔으며, 왜 이러한 결과가 출력되었는지 설명해 봅시다. (정답은 없습니다)   \n",
    "   \n",
    "   \n",
    "### 프로젝트 1-2.\n",
    "\n",
    "1. (적당히) 낮은 해상도를 가진 이미지를 검색해서 한 장 고른 후 저장하고 불러옵니다.\n",
    "2. 불러온 이미지를 입력으로 SRGAN을 이용해 고해상도 이미지를 생성합니다. 이전에 사용한 apply_srgan 함수를 사용하면 쉽습니다.\n",
    "4. 1.에서 불러온 이미지에 bicubic interpolation을 적용해 가로 및 세로 픽셀 수를 다시 4배로 늘립니다. cv2.resize()를 사용해 봅시다.\n",
    "4. 2개 이미지(3.의 Bicubic의 결과, 2.의 SRGAN의 결과)를 나란히 시각화합니다. 각 이미지의 제목에 어떤 방법에 대한 결과인지 표시해 주세요. 이전 시각화에 사용했던 코드를 참고하면 어렵지 않습니다.\n",
    "5. 선택한 이미지를 DIV2K 데이터셋에서 학습된 모델로 Super Resolution했을 때 어떠한 결과가 나왔으며, 왜 이러한 결과가 출력되었는지 설명해 봅시다. (정답은 없습니다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "train, valid = tfds.load(\n",
    "    \"div2k/bicubic_x4\", \n",
    "    split=[\"train\",\"validation\"],\n",
    "    as_supervised=True\n",
    ")\n",
    "def preprocessing(lr, hr):\n",
    "    hr = tf.cast(hr, tf.float32) /255.\n",
    "        \n",
    "    # 이미지의 크기가 크므로 (96,96,3) 크기로 임의 영역을 잘라내어 사용합니다.\n",
    "    hr_patch = tf.image.random_crop(hr, size=[96,96,3])\n",
    "        \n",
    "    # 잘라낸 고해상도 이미지의 가로, 세로 픽셀 수를 1/4배로 줄입니다\n",
    "    # 이렇게 만든 저해상도 이미지를 SRGAN의 입력으로 사용합니다.\n",
    "    lr_patch = tf.image.resize(hr_patch, [96//4, 96//4], \"bicubic\")\n",
    "    return lr_patch, hr_patch\n",
    "\n",
    "train = train.map(preprocessing).shuffle(buffer_size=10).repeat().batch(8)\n",
    "valid = valid.map(preprocessing).repeat().batch(8)\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input, Model\n",
    "\n",
    "# 그림의 파란색 블록을 정의합니다.\n",
    "def gene_base_block(x):\n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(x)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.PReLU(shared_axes=[1,2])(out)\n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    return layers.Add()([x, out])\n",
    "\n",
    "# 그림의 뒤쪽 연두색 블록을 정의합니다.\n",
    "def upsample_block(x):\n",
    "    out = layers.Conv2D(256, 3, 1, \"same\")(x)\n",
    "    # 그림의 PixelShuffler 라고 쓰여진 부분을 아래와 같이 구현합니다.\n",
    "    out = layers.Lambda(lambda x: tf.nn.depth_to_space(x, 2))(out)\n",
    "    return layers.PReLU(shared_axes=[1,2])(out)\n",
    "    \n",
    "# 전체 Generator를 정의합니다.\n",
    "def get_generator(input_shape=(None, None, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    out = layers.Conv2D(64, 9, 1, \"same\")(inputs)\n",
    "    out = residual = layers.PReLU(shared_axes=[1,2])(out)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        out = gene_base_block(out)\n",
    "    \n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Add()([residual, out])\n",
    "    \n",
    "    for _ in range(2):\n",
    "        out = upsample_block(out)\n",
    "        \n",
    "    out = layers.Conv2D(3, 9, 1, \"same\", activation=\"tanh\")(out)\n",
    "    return Model(inputs, out)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "# 그림의 파란색 블록을 정의합니다.\n",
    "def disc_base_block(x, n_filters=128):\n",
    "    out = layers.Conv2D(n_filters, 3, 1, \"same\")(x)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    out = layers.Conv2D(n_filters, 3, 2, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    return layers.LeakyReLU()(out)\n",
    "\n",
    "# 전체 Discriminator 정의합니다.\n",
    "def get_discriminator(input_shape=(None, None, 3)):\n",
    "    inputs = Input(input_shape)\n",
    "    \n",
    "    out = layers.Conv2D(64, 3, 1, \"same\")(inputs)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    out = layers.Conv2D(64, 3, 2, \"same\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    \n",
    "    for n_filters in [128, 256, 512]:\n",
    "        out = disc_base_block(out, n_filters)\n",
    "    \n",
    "    out = layers.Dense(1024)(out)\n",
    "    out = layers.LeakyReLU()(out)\n",
    "    out = layers.Dense(1, activation=\"sigmoid\")(out)\n",
    "    return Model(inputs, out)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras import applications\n",
    "def get_feature_extractor(input_shape=(None, None, 3)):\n",
    "    vgg = applications.vgg19.VGG19(\n",
    "        include_top=False, \n",
    "        weights=\"imagenet\", \n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    # 아래 vgg.layers[20]은 vgg 내의 마지막 convolutional layer 입니다.\n",
    "    return Model(vgg.input, vgg.layers[20].output)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
